import model_loader
import pipeline 
from PIL import image 
from transformers import CLIPTokenizer 
import torch

DEVICE = "cpu"

ALLOW_CUDA = True
ALLOW_MPS = False

if torch.cuda.is_available() and ALLOW_CUDA:
	DEVICE = "cuda"
elif (torch.has_mps or torch.backend.mps.is_available()) and ALLOW_MPS:
	DEVICE = "mps"
print(f"using device {DEVICE}")

tokenizer = CLIPTokenizer("../data/vocab.json", merges_file="../data/merges.txt")
model_file = "./data/v1-5-pruned-emaonly.ckpt"
models = model_loader.preload_models_from_strandard_weights(model_file, DEVICE)

#text to image 

prompt = "a cat stretching on the floor, highly detailed, ultra sharp, cinemtatic"
uncond_prompt = " " #negative prompt 
do_cfg = True
cfg_scale = 7

# image to image 

input_image = None
image_path = "./images/dog.jpg"

#input_image = Image.open(image_path)
strength = 0.9

sampler = "ddpm"
num_inference_steps = 50
seed = 42

output_image = pipeline.generate(
	prompt = prompt,
	uncond_prompt = uncond_prompt,
	input_image = input_image,
	strength = strength,
	do_cfg = do_cfg,
	cfg_scale = cfg_scale,
	sampler_name = sampler,
	n_inference_steps = num_inference_steps,
	seed = seed,
	models = models,
	device = DEVICE,
	idle_device = "cpu",
	tokenizer = tokenizer 
)

Image.fromarray(output_image)
